{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2C8V1skstgvC"},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# We assume you uploaded the exercise folder in root Google Drive folder\n","!cp -r /content/drive/MyDrive/3d-lmnet-pytorch 3d-lmnet-pytorch\n","os.chdir('/content/drive/MyDrive/3d-lmnet-pytorch')\n","print('Installing requirements')\n","!pip install -r requirements.txt\n","\n","# Make sure you restart runtime when directed by Colab"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1673119563039,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"9jmMRX90tj2P","outputId":"b8fc1fb2-ec8e-48f7-8690-e4b95a92279a"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA availability: True\n"]}],"source":["import os\n","import sys\n","import torch\n","os.chdir('/content/3d-lmnet-pytorch/3d-lmnet-pytorch')\n","sys.path.insert(1, \"/content/3d-lmnet-pytorch/3d-lmnet-pytorch\")\n","print('CUDA availability:', torch.cuda.is_available())"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":257,"status":"ok","timestamp":1673119565346,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"eXCyGAKZtrLd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"47b40be5-6a67-4926-84bb-2cc3c0a86888"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","from pathlib import Path\n","import numpy as np\n","import matplotlib as plt\n","import k3d\n","import trimesh\n","import torch\n","import skimage"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1673119567389,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"9mlePuTzttf0","outputId":"508236bb-befb-4e38-b566-2642231259ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":21}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"zUT-2sxV3ojS"},"source":["## ShapeNet Terms and Conditions\n","\n","In order to be able to use the data, we agree the below terms and conditions:\n","\n","1. Researcher shall use the Database only for non-commercial research and educational purposes.\n","2. Princeton University and Stanford University make no representations or warranties regarding the Database, including but not limited to warranties of non-infringement or fitness for a particular purpose.\n","3. Researcher accepts full responsibility for his or her use of the Database and shall defend and indemnify Princeton University and Stanford University, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the Database, including but not limited to Researcher's use of any copies of copyrighted 3D models that he or she may create from the Database.\n","4. Researcher may provide research associates and colleagues with access to the Database provided that they first agree to be bound by these terms and conditions.\n","5. Princeton University and Stanford University reserve the right to terminate Researcher's access to the Database at any time.\n","6. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer.\n","7. The law of the State of New Jersey shall apply to all disputes under this agreement."]},{"cell_type":"markdown","metadata":{"id":"aU6UFVmB2sDw"},"source":["### Unzip ShapeNet pointcloud zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHN_mJLtJYkz"},"outputs":[],"source":["!unzip -q ./data/ShapeNet_pointclouds.zip -d ./data"]},{"cell_type":"markdown","metadata":{"id":"gnY06WQ92yzn"},"source":["### Download 2D images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":789300,"status":"ok","timestamp":1671320664113,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"WGwtTRnW40w_","outputId":"27f148a6-f807-42ad-da5c-8e4cfed5173b"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-12-17 23:31:14--  http://cvgl.stanford.edu/data2/ShapeNetRendering.tgz\n","Resolving cvgl.stanford.edu (cvgl.stanford.edu)... 171.64.64.64\n","Connecting to cvgl.stanford.edu (cvgl.stanford.edu)|171.64.64.64|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://cvgl.stanford.edu/data2/ShapeNetRendering.tgz [following]\n","--2022-12-17 23:31:14--  https://cvgl.stanford.edu/data2/ShapeNetRendering.tgz\n","Connecting to cvgl.stanford.edu (cvgl.stanford.edu)|171.64.64.64|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12318245442 (11G) [application/x-gzip]\n","Saving to: ‘/content/term_project/data/ShapeNetRendering.tgz’\n","\n","ShapeNetRendering.t 100%[===================>]  11.47G  13.1MB/s    in 13m 8s  \n","\n","2022-12-17 23:44:23 (14.9 MB/s) - ‘/content/term_project/data/ShapeNetRendering.tgz’ saved [12318245442/12318245442]\n","\n"]}],"source":["!wget http://cvgl.stanford.edu/data2/ShapeNetRendering.tgz -P ./data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdDWzJHLAcDz"},"outputs":[],"source":["!tar -xf ./data/ShapeNetRendering.tgz -C ./data\n","#!rm /content/term-project/data/ShapeNetRendering.tgz"]},{"cell_type":"markdown","metadata":{"id":"9WpwE3tS22bA"},"source":["### Construct ShapeNet dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1106,"status":"ok","timestamp":1673042414038,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"pKdGd1kvv7ub","outputId":"410e8d80-5d44-44f7-9722-050a06731ca1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of train set: 26271\n","Length of val set: 8758\n","Length of test set: 8755\n"]}],"source":["from data.shapenet import ShapeNet\n","\n","# Create a dataset with train split\n","train_dataset = ShapeNet('train')\n","val_dataset = ShapeNet('valid')\n","#overfit_dataset = ShapeNet('overfit')\n","test_dataset = ShapeNet('test')\n","\n","# Get length, which is a call to __len__ function\n","print(f'Length of train set: {len(train_dataset)}') \n","# Get length, which is a call to __len__ function\n","print(f'Length of val set: {len(val_dataset)}') \n","# Get length, which is a call to __len__ function\n","print(f'Length of test set: {len(test_dataset)}')  "]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":258,"status":"ok","timestamp":1673119581719,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"-enEihrj4z-T","outputId":"bdff13f9-d6f0-4682-a48e-b420bc4c152b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input images: (24, 137, 3, 137)\n","Input point cloud: (2048, 3)\n"]}],"source":["\n","from skimage.measure import marching_cubes\n","\n","train_sample = train_dataset[1]\n","print(f'Input images: {train_sample[\"img\"].shape}')  \n","print(f'Input point cloud: {train_sample[\"point\"].shape}')  "]},{"cell_type":"markdown","metadata":{"id":"_s5meRr62_S1"},"source":["### Print output shape of the 2D Encoder model (both variational and normal versions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":310,"status":"error","timestamp":1673042434619,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"W5zhCIIZlEui","outputId":"e86476d1-5b86-48c5-d950-9eeb2cf35405"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0b9aacc72ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_2d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel2d_variational\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"variational\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m137\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m137\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/3d-lmnet-pytorch/3d-lmnet-pytorch/model/model_2d.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, final_layer, bottleneck)\u001b[0m\n\u001b[1;32m      8\u001b[0m         self.base = nn.Sequential(\n\u001b[1;32m      9\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mpadding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mdilation_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         super(Conv2d, self).__init__(\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m     98\u001b[0m                         padding, valid_padding_strings))\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'same'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"padding='same' is not supported for strided convolutions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mvalid_padding_modes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reflect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replicate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'circular'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: padding='same' is not supported for strided convolutions"]}],"source":["from model.model_2d import ImageEncoder\n","\n","model2d_variational=ImageEncoder(\"variational\",512)\n","\n","input_tensor = torch.randn(137,137,3)\n","mu,std = model2d_variational(input_tensor)\n","print(\"Mu:\",mu,\"Std:\",std)\n","\n","model2d_normal=ImageEncoder(\"normal\",512)\n","\n","latent=model2d_normal(input_tensor)\n","print(\"Latent shape:\",latent.shape)"]},{"cell_type":"markdown","metadata":{"id":"XhQwt86S3KLX"},"source":["### Train 2D Encoder model to match the predicted latent space to the output of 3D Encoder of pointclouds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"elapsed":526,"status":"error","timestamp":1673042443823,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"H5AfhbLEnbIt","outputId":"419ba570-6081-45c8-bde8-9fd321815170"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-5ed24ae0dd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel2d_variational\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneralization_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model2d_variational' is not defined"]}],"source":["from training import train_2d_to_3d\n","\n","generalization_config = {\n","    'experiment_name': '2d_to_3d_variational',\n","    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n","    'is_overfit': False,\n","    'bottleneck': 512,\n","    'batch_size': 32,\n","    'resume_ckpt': None,\n","    'learning_rate_model':  0.00005,\n","    'max_epochs': 30,  \n","    'print_every_n': 5,\n","    'visualize_every_n': 5,\n","}\n","\n","model2d_variational.main(generalization_config)"]},{"cell_type":"markdown","metadata":{"id":"BMfuS_dL3T-A"},"source":["### Infer pointclouds using trained 2D Encoder and 3D Decoder models"]},{"cell_type":"markdown","metadata":{"id":"NFRpLXYm3ef-"},"source":["Variational inferences:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1KCfUPQoRIT"},"outputs":[],"source":["from inference.infer_2d_to_3d import Inference2DToPointCloudVariational\n","\n","device = torch.device('cuda:0')  # change this to cpu if you're not using a gpu\n","id=torch.randint(0,len(test_dataset))\n","val_config={\"final_layer\" : \"variational\",\n","        \"bottleneck\" : 512,\n","        \"input_size\" : None,\n","        \"hidden_size\" : None,\n","        \"output_size\" : None,\n","        \"bnorm\" : True,\n","        \"bnorm_final\" : False,\n","        \"regularizer\" : None,\n","        \"weight_decay\" : 0.001,\n","        \"dropout_prob\" : None}\n","Inference2DToPointCloudVariational(test_dataset[id],\"content/3d-lmnet-pytorch/runs/2d_to_3d_variational\",\"content/3d-lmnet-pytorch/runs/3d_pointcloud_decoder\", val_config,device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6fnyhF1pPSo"},"outputs":[],"source":["predicted_point_clouds=inference_handler_variational.infer()"]},{"cell_type":"markdown","metadata":{"id":"xa-wqJSr3hk-"},"source":["Normal inferences:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRDWqdt1vZ5d"},"outputs":[],"source":["from inference.infer_2d_to_3d import Inference2DToPointCloudNormal\n","\n","id=torch.randint(0,len(test_dataset))\n","val_config={\"final_layer\" : \"normal\",\n","        \"bottleneck\" : 512,\n","        \"input_size\" : None,\n","        \"hidden_size\" : None,\n","        \"output_size\" : None,\n","        \"bnorm\" : True,\n","        \"bnorm_final\" : False,\n","        \"regularizer\" : None,\n","        \"weight_decay\" : 0.001,\n","        \"dropout_prob\" : None}\n","Inference2DToPointCloudNormal(test_dataset[id],\"content/3d-lmnet-pytorch/runs/2d_to_3d_normal\", \"content/3d-lmnet-pytorch/runs/3d_pointcloud_decoder\",val_config,device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77ic7G8_vm0f"},"outputs":[],"source":["predicted_point_cloud=inference_handler_normal.infer()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"elapsed":423,"status":"error","timestamp":1673042489588,"user":{"displayName":"ml3d dai","userId":"13876971419975352583"},"user_tz":-60},"id":"3dU0c36AFxVJ","outputId":"31eb1a02-9d92-42b9-9556-533e876074ba"},"outputs":[{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-b0a0f9370335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_ae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneralization_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/3d-lmnet-pytorch/3d-lmnet-pytorch/training/train_ae.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/3d-lmnet-pytorch/3d-lmnet-pytorch/model/model_3d_autoencoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'input_size', 'hidden_size', and 'output_size'"]}],"source":["from training import train_ae\n","\n","\n","    # parser.add_argument(\"--root\", type=str, default=\"./data\")\n","    # parser.add_argument(\"--npoints\", type=int, default=2048)\n","    # parser.add_argument(\"--mpoints\", type=int, default=2025)\n","    # parser.add_argument(\"--batch_size\", type=int, default=16)\n","    # parser.add_argument(\"--lr\", type=float, default=1e-4)\n","    # parser.add_argument(\"--weight_decay\", type=float, default=1e-6)\n","    # parser.add_argument(\"--epochs\", type=int, default=400)\n","    # parser.add_argument(\"--num_workers\", type=int, default=4)\n","    # parser.add_argument(\"--log_dir\", type=str, default=\"./log\")\n","\n","generalization_config = {\n","    'root': './3d-lmnet-pytorch/',\n","    'experiment_name': '3d_autoencoder',\n","    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n","    'is_overfit': False,\n","    'npoints': 2048,\n","    'mpoints': 2025,\n","    'lr': 1e-4,\n","    'weight_decay': 1e-6,\n","    'bottleneck': 512,\n","    'batch_size': 16,\n","    'resume_ckpt': None,\n","    'learning_rate_model':  0.00005,\n","    'max_epochs': 400,  \n","    'num_workers': 4,\n","    \"input_size\" : 256,\n","    \"hidden_size\" : 256,\n","    \"output_size\" : 256,\n","    'log_dir': './3d-lmnet-pytorch/',\n","    'print_every_n': 5,\n","    'visualize_every_n': 5,\n","}\n","\n","train_ae.main(generalization_config)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"1a453b615765addad4971118d8a9ab96c4ea1423c3e71d4bd1e873af4a20b25f"}}},"nbformat":4,"nbformat_minor":0}