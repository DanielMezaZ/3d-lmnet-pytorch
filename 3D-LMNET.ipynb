{"cells":[{"cell_type":"markdown","metadata":{"id":"P0FrnfMcAsrr"},"source":["# Imports and Setup"]},{"cell_type":"markdown","metadata":{"id":"d1FIpkfB6Y-R"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2C8V1skstgvC"},"outputs":[],"source":["import os\n","\n","!git clone https://github.com/zcanfes/3d-lmnet-pytorch.git\n","os.chdir('/content/3d-lmnet-pytorch')\n","\n","print('Installing requirements')\n","!pip install -r requirements.txt\n","\n","# Make sure you restart runtime when directed by Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jmMRX90tj2P"},"outputs":[],"source":["import sys\n","import torch\n","sys.path.insert(1, \"/content/3d-lmnet-pytorch/3d-lmnet-pytorch\")\n","print('CUDA availability:', torch.cuda.is_available())\n","%cd /content/3d-lmnet-pytorch/3d-lmnet-pytorch"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675717854279,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"eXCyGAKZtrLd"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","from pathlib import Path\n","import numpy as np\n","import matplotlib as plt\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mlePuTzttf0"},"outputs":[],"source":["torch.cuda.is_available()\n","need_pytorch3d=False\n","try:\n","    import pytorch3d\n","except ModuleNotFoundError:\n","    need_pytorch3d=True\n","if need_pytorch3d:\n","    if sys.platform.startswith(\"linux\"):\n","        # We try to install PyTorch3D via a released wheel.\n","        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n","        version_str=\"\".join([\n","            f\"py3{sys.version_info.minor}_cu\",\n","            torch.version.cuda.replace(\".\",\"\"),\n","            f\"_pyt{pyt_version_str}\"\n","        ])\n","        !pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n","    else:\n","        # We try to install PyTorch3D from source.\n","        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n","        !tar xzf 1.10.0.tar.gz\n","        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n","        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"]},{"cell_type":"markdown","metadata":{"id":"rxKfykIdzj3h"},"source":["## Prepare Datasets\n","**Important Note:** Before running these cells, you should make sure that you downloaded the datasets and moved them to the /data folder."]},{"cell_type":"markdown","metadata":{"id":"zUT-2sxV3ojS"},"source":["### ShapeNet Terms and Conditions\n","\n","In order to be able to use the data, we agree the below terms and conditions:\n","\n","1. Researcher shall use the Database only for non-commercial research and educational purposes.\n","2. Princeton University and Stanford University make no representations or warranties regarding the Database, including but not limited to warranties of non-infringement or fitness for a particular purpose.\n","3. Researcher accepts full responsibility for his or her use of the Database and shall defend and indemnify Princeton University and Stanford University, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the Database, including but not limited to Researcher's use of any copies of copyrighted 3D models that he or she may create from the Database.\n","4. Researcher may provide research associates and colleagues with access to the Database provided that they first agree to be bound by these terms and conditions.\n","5. Princeton University and Stanford University reserve the right to terminate Researcher's access to the Database at any time.\n","6. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer.\n","7. The law of the State of New Jersey shall apply to all disputes under this agreement."]},{"cell_type":"markdown","metadata":{"id":"aU6UFVmB2sDw"},"source":["### Unzip ShapeNet pointcloud zip"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":60428,"status":"ok","timestamp":1675718057609,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"yHN_mJLtJYkz"},"outputs":[],"source":["!unzip -q ./data/ShapeNet_pointclouds.zip -d ./data"]},{"cell_type":"markdown","metadata":{"id":"gnY06WQ92yzn"},"source":["### Download 2D images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGwtTRnW40w_"},"outputs":[],"source":["!wget http://cvgl.stanford.edu/data2/ShapeNetRendering.tgz -P ./data"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":214445,"status":"ok","timestamp":1675719169383,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"BdDWzJHLAcDz"},"outputs":[],"source":["!tar -xf ./data/ShapeNetRendering.tgz -C ./data\n","#!rm /content/term-project/data/ShapeNetRendering.tgz"]},{"cell_type":"markdown","metadata":{"id":"9WpwE3tS22bA"},"source":["### Construct ShapeNet dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1272,"status":"ok","timestamp":1675719959419,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"pKdGd1kvv7ub","outputId":"07079c85-be5e-47e7-8275-63e18734adf8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset is prepared for 2D model and variant 1:\n","Length of train set: 630504\n","Length of val set: 210192\n","Length of test set: 210120\n","\n","Dataset is prepared for 2D model and variant 2 (only chair class):\n","Length of train set: 97608\n","Length of val set: 32544\n","Length of test set: 32520\n","\n","Dataset is prepared for 3D autoencoder:\n","Length of train set: 26271\n","Length of val set: 8758\n","Length of test set: 8755\n"]}],"source":["from data.shapenet import ShapeNet\n","\n","# Create a dataset with train split\n","train_dataset = ShapeNet('train',image_model=True)\n","val_dataset = ShapeNet('valid',image_model=True)\n","test_dataset = ShapeNet('test',image_model=True)\n","\n","print(\"Dataset is prepared for 2D model and variant 1:\")\n","print(f'Length of train set: {len(train_dataset)}') \n","print(f'Length of val set: {len(val_dataset)}') \n","print(f'Length of test set: {len(test_dataset)}')  \n","\n","# Create a dataset with train split\n","train_dataset = ShapeNet('train',image_model=True,cat=1)\n","val_dataset = ShapeNet('valid',image_model=True,cat=1)\n","test_dataset = ShapeNet('test',image_model=True,cat=1)\n","\n","print(\"\\nDataset is prepared for 2D model and variant 2 (only chair class):\")\n","print(f'Length of train set: {len(train_dataset)}') \n","print(f'Length of val set: {len(val_dataset)}') \n","print(f'Length of test set: {len(test_dataset)}')  \n","\n","train_dataset = ShapeNet('train')\n","val_dataset = ShapeNet('valid')\n","test_dataset = ShapeNet('test')\n","\n","print(\"\\nDataset is prepared for 3D autoencoder:\")\n","print(f'Length of train set: {len(train_dataset)}') \n","print(f'Length of val set: {len(val_dataset)}') \n","print(f'Length of test set: {len(test_dataset)}')  "]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1675719963857,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"-enEihrj4z-T","outputId":"e297b890-b082-415e-eb2f-0b6c7c403a47"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input images: (3, 128, 128)\n","Input point cloud: (2048, 3)\n"]}],"source":["train_sample = train_dataset[1]\n","print(f'Input images: {train_sample[\"img\"].shape}')  \n","print(f'Input point cloud: {train_sample[\"point\"].shape}')  "]},{"cell_type":"markdown","metadata":{"id":"_s5meRr62_S1"},"source":["### Print output shape of the 2D Encoder model (both Variant I and Variant II versions)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13463,"status":"ok","timestamp":1675719979910,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"W5zhCIIZlEui","outputId":"970f07e3-e9a0-4b17-e8bd-44d89b1db381"},"outputs":[{"name":"stdout","output_type":"stream","text":["input size:  torch.Size([1, 3, 128, 128])\n","Mu size:  torch.Size([1, 512]) , Std size:  torch.Size([1, 512])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 128, 128]             896\n","              ReLU-2         [-1, 32, 128, 128]               0\n","            Conv2d-3         [-1, 32, 128, 128]           9,248\n","              ReLU-4         [-1, 32, 128, 128]               0\n","            Conv2d-5           [-1, 64, 63, 63]          18,496\n","              ReLU-6           [-1, 64, 63, 63]               0\n","            Conv2d-7           [-1, 64, 63, 63]          36,928\n","              ReLU-8           [-1, 64, 63, 63]               0\n","            Conv2d-9           [-1, 64, 63, 63]          36,928\n","             ReLU-10           [-1, 64, 63, 63]               0\n","           Conv2d-11          [-1, 128, 31, 31]          73,856\n","             ReLU-12          [-1, 128, 31, 31]               0\n","           Conv2d-13          [-1, 128, 31, 31]         147,584\n","             ReLU-14          [-1, 128, 31, 31]               0\n","           Conv2d-15          [-1, 128, 31, 31]         147,584\n","             ReLU-16          [-1, 128, 31, 31]               0\n","           Conv2d-17          [-1, 256, 15, 15]         295,168\n","             ReLU-18          [-1, 256, 15, 15]               0\n","           Conv2d-19          [-1, 256, 15, 15]         590,080\n","             ReLU-20          [-1, 256, 15, 15]               0\n","           Conv2d-21          [-1, 256, 15, 15]         590,080\n","             ReLU-22          [-1, 256, 15, 15]               0\n","           Conv2d-23            [-1, 512, 7, 7]       1,180,160\n","             ReLU-24            [-1, 512, 7, 7]               0\n","           Conv2d-25            [-1, 512, 7, 7]       2,359,808\n","             ReLU-26            [-1, 512, 7, 7]               0\n","           Conv2d-27            [-1, 512, 7, 7]       2,359,808\n","             ReLU-28            [-1, 512, 7, 7]               0\n","           Conv2d-29            [-1, 512, 7, 7]       2,359,808\n","             ReLU-30            [-1, 512, 7, 7]               0\n","           Conv2d-31            [-1, 512, 2, 2]       6,554,112\n","             ReLU-32            [-1, 512, 2, 2]               0\n","           Linear-33                  [-1, 512]       1,049,088\n","           Linear-34                  [-1, 512]       1,049,088\n","================================================================\n","Total params: 18,858,720\n","Trainable params: 18,858,720\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.19\n","Forward/backward pass size (MB): 37.47\n","Params size (MB): 71.94\n","Estimated Total Size (MB): 109.59\n","----------------------------------------------------------------\n","Latent shape:  torch.Size([1, 512])\n"]}],"source":["from model.model_2d import ImageEncoder\n","from torchsummary import summary\n","\n","model2d_variational=ImageEncoder(\"variational\",512)\n","model2d_variational.cuda()\n","input_tensor = torch.randn(1,3,128,128)\n","input_tensor=input_tensor.cuda()\n","print(\"input size: \",input_tensor.size())\n","\n","\n","mu,std = model2d_variational(input_tensor)\n","print(\"Mu size: \", mu.size(),\", Std size: \", std.size())\n","summary(model2d_variational,(3,128,128))\n","model2d_normal=ImageEncoder(\"normal\",512)\n","model2d_normal.cuda()\n","latent=model2d_normal(input_tensor)\n","print(\"Latent shape: \",latent.size())"]},{"cell_type":"markdown","metadata":{"id":"B28DA-Zp9Su8"},"source":["# 3D Point Cloud Autoencoder Training\n","Train the 3D point cloud autoencoder to learn to reconstruct a given point cloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5FWCyHMjRHl"},"outputs":[],"source":["# Define the folder you want to save your trained model to\n","AUTOENCODER_EXP = \"3d_autoencoder\"\n","!mkdir \"{AUTOENCODER_EXP}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43FQyYb9-XJE"},"outputs":[],"source":["from training import train_ae\n","\n","config = {\n","    'log_dir': f\"./{AUTOENCODER_EXP}\",\n","    'device': 'cuda:0', \n","    'lr': 1e-4,\n","    'weight_decay': 1e-6,\n","    'batch_size': 32,\n","    'resume_ckpt': None,\n","    'learning_rate_model':  5e-5,\n","    'max_epochs': 500,  \n","    'num_workers': 4,\n","    'bottleneck': 512,\n","    \"hidden_size\" : 256,\n","    \"output_size\" : 2048*3,\n","    'print_every_n': 5,\n","    'visualize_every_n': 5,\n","}\n","\n","train_ae.main(config)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XhQwt86S3KLX"},"source":["# 2D Image Encoder Training\n","Train 2D Image Encoder model to match the predicted latent space to the output of 3D Encoder of pointclouds.\n","\n","**Important Note:** Below training cells are arranged to used the newly trained autoencoder. If you want to use our pre-trained autoencoder model instead, you should change `\"3d_autoencoder_path\"` field in the config by running the following cell."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":351,"status":"ok","timestamp":1675720377314,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"OhOhYkpds323"},"outputs":[],"source":["AUTOENCODER_EXP = \"trained_models\""]},{"cell_type":"markdown","metadata":{"id":"uvuWag580_fw"},"source":["## Variant I - Latent Matching with L1 Loss"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":667,"status":"ok","timestamp":1675720397564,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"Nv0mv7Jnto2T"},"outputs":[],"source":["ENCODER_EXP = \"2d_encoder\"\n","!mkdir \"{ENCODER_EXP}\""]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":713,"status":"ok","timestamp":1675720399712,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"DRtTmZLcjRHm"},"outputs":[],"source":["!mkdir \"{ENCODER_EXP}/L1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5AfhbLEnbIt"},"outputs":[],"source":["from training import train_2d_to_3d\n","\n","config = {\n","    'experiment_name': f\"./{ENCODER_EXP}/L1\",\n","    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n","    'bottleneck': 512,\n","    \"cat\":13,\n","    'batch_size': 32,\n","    \"loss_criterion\":\"L1\",\n","    \"final_layer\":\"normal\",\n","    \"3d_autoencoder_path\": f\"./{AUTOENCODER_EXP}/model_autoencoder_final.pth\",\n","    'resume_ckpt': None,\n","    'learning_rate_model': 5e-5,\n","    'max_epochs': 30,  \n","    'save_every_n': 1,\n","    'validate_every_n': 3,\n","    \"autoencoder_bottleneck\":512,\n","    \"autoencoder_hidden_size\":256,\n","    \"autoencoder_output_size\":2048*3,\n","}\n","\n","train_2d_to_3d.main(config)"]},{"cell_type":"markdown","metadata":{"id":"0UT6VzUJ1TKB"},"source":["## Variant I - Latent Matching with L2 Loss"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":714,"status":"ok","timestamp":1675720792950,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"Q2NoXMHljRHn"},"outputs":[],"source":["!mkdir \"{ENCODER_EXP}/L2\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHNSisYe1VFg"},"outputs":[],"source":["from training import train_2d_to_3d\n","\n","config = {\n","    'experiment_name': f\"./{ENCODER_EXP}/L2\",\n","    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n","    'bottleneck': 512,\n","    \"cat\":13,\n","    'batch_size': 32,\n","    \"loss_criterion\":\"L2\",\n","    \"final_layer\":\"normal\",\n","    \"3d_autoencoder_path\": f\"./{AUTOENCODER_EXP}/model_autoencoder_final.pth\",\n","    'resume_ckpt': None,\n","    'learning_rate_model':  5e-5,\n","    'max_epochs': 30,  \n","    'save_every_n': 1,\n","    'validate_every_n': 3,\n","    \"autoencoder_bottleneck\":512,\n","    \"autoencoder_hidden_size\":256,\n","    \"autoencoder_output_size\":2048*3,\n","}\n","\n","train_2d_to_3d.main(config)"]},{"cell_type":"markdown","metadata":{"id":"cVjsNa1N1px7"},"source":["## Variant II - Probabilistic Latent Matching"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":377,"status":"ok","timestamp":1675720833891,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"R-cjA7QYjRHp"},"outputs":[],"source":["!mkdir \"{ENCODER_EXP}/DIV\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xv-oEiXO1r0F"},"outputs":[],"source":["from training import train_2d_to_3d\n","\n","config = {\n","    'experiment_name': f\"./{ENCODER_EXP}/DIV\",\n","    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n","    'bottleneck': 512,\n","    \"cat\":1,\n","    'batch_size': 32,\n","    \"loss_criterion\":\"variational\",\n","    \"final_layer\":\"variational\",\n","    \"3d_autoencoder_path\": f\"./{AUTOENCODER_EXP}/model_autoencoder_final.pth\",\n","    'resume_ckpt': None,\n","    'learning_rate_model':  5e-5,\n","    'max_epochs': 20,  \n","    'save_every_n': 1,\n","    'validate_every_n': 3,\n","    \"autoencoder_bottleneck\":512,\n","    \"autoencoder_hidden_size\":256,\n","    \"autoencoder_output_size\":2048*3,\n","    \"alpha\":0.2,\n","    \"penalty_angle\":20,\n","    \"lambda\":5.5\n","}\n","\n","train_2d_to_3d.main(config)"]},{"cell_type":"markdown","metadata":{"id":"BMfuS_dL3T-A"},"source":["# Inference\n","Infer pointclouds using the trained 2D Image Encoder and 3D Pointcloud Decoder models. \n","\n","**Important Note:** Before running these cells, you should download the pre-trained models and create a folder \"trained_models\" and move the models you want to use for inference to trained_models/."]},{"cell_type":"markdown","metadata":{"id":"xa-wqJSr3hk-"},"source":["## Variant I - Inferences"]},{"cell_type":"markdown","metadata":{"id":"G4iWoRro2RBv"},"source":["### 2D Image Encoder with L1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJPfz9M6jRHr"},"outputs":[],"source":["# If you want to infer with the models you trained above uncomment and run this cell\n","\n","# EXP_NAME = \"{ENCODER_EXP}/L1/model_final.pth\"\n","# AUTOENCODER_EXP = \"3d_autoencoder\""]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1675722168131,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"twQZBOvKjRHr"},"outputs":[],"source":["# If you want to infer the pre-trained models run this cell\n","EXP_NAME = \"trained_models/model_epoch_L1_30.pth\""]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":1259,"status":"ok","timestamp":1675722027537,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"sxNneVRAjRHr"},"outputs":[],"source":["# Define the folders you want to save the results to\n","\n","RESULTS = \"image_encoder_L1_results\"\n","PRED = \"pred\"\n","GT = \"gt\"\n","\n","!mkdir \"{RESULTS}\"\n","!mkdir \"{RESULTS}/{PRED}\"\n","!mkdir \"{RESULTS}/{GT}\"\n","!mkdir \"{RESULTS}/{GT}/pointcloud\"\n","!mkdir \"{RESULTS}/{GT}/image\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRDWqdt1vZ5d"},"outputs":[],"source":["from inference import inference_2d_to_3d \n","\n","config = {\n","    'encoder_path': f'./{EXP_NAME}',\n","    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n","    'bottleneck': 512,\n","    \"2d_inference_pred\":f\"./{RESULTS}/{PRED}/\",\n","    \"2d_inference_gt_point\":f\"./{RESULTS}/{GT}/pointcloud/\",\n","    \"2d_inference_gt_img\":f\"./{RESULTS}/{GT}/image/\",\n","    \"cat\":13,\n","    'batch_size': 1,\n","    \"loss_criterion\":\"L1\",\n","    \"final_layer\":\"normal\",\n","    \"3d_autoencoder_path\":f\"./{AUTOENCODER_EXP}/model_autoencoder_final.pth\",\n","    'resume_ckpt': None,\n","    'learning_rate_model':  5e-5,\n","    \"autoencoder_bottleneck\":512,\n","    \"autoencoder_hidden_size\":256,\n","    \"autoencoder_output_size\":2048*3\n","\n","}\n","inference_2d_to_3d.main(config)"]},{"cell_type":"markdown","metadata":{"id":"9-8C7ogL2Typ"},"source":["### 2D Image Encoder with L2"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":328,"status":"ok","timestamp":1675722172117,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"BrcRTiwnjRHs"},"outputs":[],"source":["# If you want to infer the model you trained above uncomment and run this cell\n","\n","# EXP_NAME = \"./{ENCODER_EXP}/L2/model_final.pth\""]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675722172930,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"8ALvA_hMjRHt"},"outputs":[],"source":["# If you want to infer the pre-trained models run this cell\n","\n","EXP_NAME = \"trained_models/model_epoch_L2_30.pth\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yn7mH1dfjRHt"},"outputs":[],"source":["# Define the folders you want to save the results to\n","\n","RESULTS = \"image_encoder_L2_results\"\n","PRED = \"pred\"\n","GT = \"gt\"\n","\n","!mkdir \"{RESULTS}\"\n","!mkdir \"{RESULTS}/{PRED}\"\n","!mkdir \"{RESULTS}/{GT}\"\n","!mkdir \"{RESULTS}/{GT}/pointcloud\"\n","!mkdir \"{RESULTS}/{GT}/image\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4I2gY2Ai2W7e"},"outputs":[],"source":["from inference import inference_2d_to_3d \n","\n","config = {\n","    'encoder_path': f'./{EXP_NAME}',\n","    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n","    'bottleneck': 512,\n","    \"2d_inference_pred\":f\"./{RESULTS}/{PRED}/\",\n","    \"2d_inference_gt_point\":f\"./{RESULTS}/{GT}/pointcloud/\",\n","    \"2d_inference_gt_img\":f\"./{RESULTS}/{GT}/image/\",\n","    \"cat\":13,\n","    'batch_size': 1,\n","    \"loss_criterion\":\"L2\",\n","    \"final_layer\":\"normal\",\n","    \"3d_autoencoder_path\":f\"./{AUTOENCODER_EXP}/model_autoencoder_final.pth\",\n","    'resume_ckpt': None,\n","    'learning_rate_model':  5e-5,\n","    \"autoencoder_bottleneck\":512,\n","    \"autoencoder_hidden_size\":256,\n","    \"autoencoder_output_size\":2048*3\n","\n","}\n","inference_2d_to_3d.main(config)"]},{"cell_type":"markdown","metadata":{"id":"zJtbYMIm9CsE"},"source":["## Variant II - Inferences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"biYZcLWmjRHu"},"outputs":[],"source":["# If you want to infer the model you trained above uncomment and run this cell\n","\n","# EXP_NAME = \"./{ENCODER_EXP}/DIV/model_final.pth\""]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1675722257734,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"2QCg3X7gjRHw"},"outputs":[],"source":["# If you want to infer the pre-trained models run this cell\n","\n","# If lambda = 5.5:\n","EXP_NAME = \"trained_models/model_epoch_30_55.pth\"\n","\n","# If lambda = 0.5:\n","# EXP_NAME = \"trained_models/model_epoch_30_05.pth\"\n","\n","# If lambda = 0.0:\n","# EXP_NAME = \"trained_models/model_epoch_30_00.pth\""]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":1475,"status":"ok","timestamp":1675722261068,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"30Dk1h7QjRHz"},"outputs":[],"source":["# Define the folders you want to save the results to\n","\n","RESULTS = \"image_encoder_DIV_results\"\n","PRED = \"pred\"\n","GT = \"gt\"\n","\n","!mkdir \"{RESULTS}\"\n","!mkdir \"{RESULTS}/{PRED}\"\n","!mkdir \"{RESULTS}/{GT}\"\n","!mkdir \"{RESULTS}/{GT}/pointcloud\"\n","!mkdir \"{RESULTS}/{GT}/image\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_9zyepD9FG2"},"outputs":[],"source":["from inference import inference_2d_to_3d \n","\n","config = {\n","    'encoder_path': f'./{EXP_NAME}',\n","    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n","    'is_overfit': False,\n","    'bottleneck': 512,\n","    \"2d_inference_pred\":f\"./{RESULTS}/{PRED}/\",\n","    \"2d_inference_gt_point\":f\"./{RESULTS}/{GT}/pointcloud/\",\n","    \"2d_inference_gt_img\":f\"./{RESULTS}/{GT}/image/\",\n","    \"cat\":1,\n","    'batch_size': 1,\n","    \"loss_criterion\":\"variational\",\n","    \"final_layer\":\"variational\",\n","    \"3d_autoencoder_path\":f\"./{AUTOENCODER_EXP}/model_autoencoder_final.pth\",\n","    'resume_ckpt': None,\n","    'learning_rate_model':  5e-5,\n","    \"autoencoder_bottleneck\":512,\n","    \"autoencoder_hidden_size\":256,\n","    \"autoencoder_output_size\":2048*3,\n","}\n","inference_2d_to_3d.main(config)"]},{"cell_type":"markdown","metadata":{"id":"V4LmVnAu2jeS"},"source":["## 3D Point Cloud Inferences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThU7VLAljRH0"},"outputs":[],"source":["# If you want to infer the model you trained above uncomment and run this cell\n","\n","# VAR_EXP_NAME = \"./{AUTOENCODER_EXP}/model_autoencoder_final.pth\""]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1675722325202,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"MjdJVS0GjRH1"},"outputs":[],"source":["# If you want to infer the pre-trained models run this cell\n","\n","EXP_NAME = \"trained_models/model_autoencoder_final.pth\""]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":1401,"status":"ok","timestamp":1675722326933,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"deVrA7PFjRH1"},"outputs":[],"source":["# Define the folders you want to save the results to\n","\n","RESULTS = \"autoencoder_results\"\n","PRED = \"pred\"\n","GT = \"gt\"\n","\n","!mkdir \"{RESULTS}\"\n","!mkdir \"{RESULTS}/{PRED}\"\n","!mkdir \"{RESULTS}/{GT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoaKVSVfgnMc"},"outputs":[],"source":["from inference import infer_3d\n","\n","config = {\n","    \"autoencoder\":f\"./{EXP_NAME}\",\n","    \"infer_gt\":f\"./{RESULTS}/{GT}/\",\n","    \"infer_pred\": f\"./{RESULTS}/{PRED}/\",\n","    'bottleneck': 512,\n","    'batch_size': 1,\n","    'num_workers': 4,\n","    \"input_size\" : 256,\n","    \"hidden_size\" : 256,\n","    \"output_size\" : 2048*3,\n","}\n","infer_3d.main(config)"]},{"cell_type":"markdown","metadata":{"id":"JzWX_iy1jRH2"},"source":["# Visualize Reconstructed Point Clouds\n","After the inference, you can use these cells to render the point clouds"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":242,"status":"ok","timestamp":1675722367725,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"8ZXXIQfD1hlD"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","# Util function for loading point clouds|\n","import numpy as np\n","\n","# Data structures and functions for rendering\n","from PIL import Image\n","from pytorch3d.structures import Pointclouds\n","from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n","from pytorch3d.renderer import (\n","    look_at_view_transform,\n","    FoVOrthographicCameras, \n","    PointsRasterizationSettings,\n","    PointsRenderer,\n","    PulsarPointsRenderer,\n","    PointsRasterizer,\n","    AlphaCompositor,\n","    NormWeightedCompositor,\n","    FoVPerspectiveCameras\n",")\n","\n","# Setup\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    torch.cuda.set_device(device)\n","else:\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":232,"status":"ok","timestamp":1675722370984,"user":{"displayName":"Zehranaz Canfes","userId":"09818614953375066911"},"user_tz":-60},"id":"GYStc037jRH3"},"outputs":[],"source":["# Initialize a camera.\n","R, T = look_at_view_transform(20, 14, -46)\n","cameras = FoVOrthographicCameras(device=device, R=R, T=T, znear=0.01)\n","\n","# Define the settings for rasterization and shading. Here we set the output image to be of size\n","# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n","# and blur_radius=0.0. Refer to rasterize_points.py for explanations of these parameters. \n","raster_settings = PointsRasterizationSettings(\n","    image_size=512, \n","    radius = 0.005,\n","    points_per_pixel = 10\n",")\n","\n","\n","# Create a points renderer by compositing points using an weighted compositor (3D points are\n","# weighted according to their distance to a pixel and accumulated using a weighted sum)\n","renderer = PointsRenderer(\n","    rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n","    compositor=NormWeightedCompositor(background_color=(255,255,255)),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoDa67rKjRH4"},"outputs":[],"source":["# Here provide the paths of reconstructed point clouds and images. The paths shouldn't contain the extension \".npy\"\n","INPUT_IMG_PATH = \"...\"\n","GT_POINT_CLOUD_PATH = \"...\"\n","PREDICTED_POINT_CLOUD_PATH = \"...\"\n","\n","file_names = [GT_POINT_CLOUD_PATH, PREDICTED_POINT_CLOUD_PATH]\n","\n","# Load and save the input image\n","image = np.load(f\"{INPUT_IMG_PATH}.npy\")\n","im = Image.fromarray(image[0].transpose(1,2,0).astype(np.uint8))\n","im.save(f\"{INPUT_IMG_PATH}.png\")\n","\n","\n","# Load point clouds\n","for name in file_names:\n","  input = np.load(f'{name}.npy')\n","\n","  if input.shape[2]!=3:\n","    input=input.transpose(0,2,1)\n","\n","  x = input[0,:, 0]\n","  y = input[0,:, 1]\n","  z = input[0,:, 2]\n","\n","  pts = np.stack((y,-x,z), axis = 1) \n","  verts = torch.Tensor(pts).to(device)\n","  # You can change the color of point clouds here\n","  rgb =  torch.tensor([72.45,251.85,528]) * torch.ones(pts.shape) / 1000.\n","\n","  point_cloud = Pointclouds(points=[verts], features=[rgb])\n","\n","  images = renderer(point_cloud)\n","  plt.figure(figsize=(9, 11))\n","  plt.imshow(images[0, ..., :3].cpu().numpy())\n","  plt.axis(\"off\");\n","  plt.tight_layout()\n","  # Save rendered point cloud image\n","  plt.savefig(f'{name}.png', dpi=300)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"892f98484189c516172582faedb5388d3b709698673dc3c1e42f631344b0864d"}}},"nbformat":4,"nbformat_minor":0}
